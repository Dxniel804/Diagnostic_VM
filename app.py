"""
Sistema de An√°lise Estrat√©gica de Follow-ups de Vendas
Automa√ß√£o que l√™ planilhas Excel e gera estrat√©gias personalizadas usando IA
"""

import os
import time
import logging
import hashlib
import pandas as pd
from flask import Flask, render_template, request, flash, redirect, url_for, session, make_response
from google import genai
from google.genai import types as genai_types
from dotenv import load_dotenv
<<<<<<< HEAD
from google import genai
=======
>>>>>>> abfb31a000e8d1a8eec3bdd4dbef1827aba10fb4
import tempfile
from werkzeug.utils import secure_filename
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, KeepTogether
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from datetime import datetime, timedelta
import json
import pickle
from PyPDF2 import PdfReader
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# Cores da Vendamais
VM_GREEN = colors.HexColor('#006400')  # Verde escuro
VM_ORANGE = colors.HexColor('#FF8C00') # Laranja
import io

# Carrega vari√°veis de ambiente
load_dotenv()

# ==================== CONFIGURA√á√ÉO ====================
app = Flask(__name__)
app.secret_key = os.getenv('SECRET_KEY', 'diagnostic_vm_secret_key_2024_persistent')
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max
app.config['SESSION_PERMANENT'] = True
app.config['SESSION_USE_SIGNER'] = True
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=24)

# Custom Jinja2 tests and filters
@app.template_test('contains')
def contains_test(value, search_term):
    """Check if value contains search term (case-insensitive)"""
    if value is None:
        return False
    return search_term.lower() in str(value).lower()

@app.template_filter('default_if_none')
def default_if_none_filter(value, default):
    """Return default value if value is None"""
    return default if value is None else value  

ALLOWED_EXTENSIONS = {'xlsx', 'xls', 'csv'}

# Cache em arquivo para relat√≥rios (solu√ß√£o para problema de sess√£o)
CACHE_DIR = 'cache'
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

def salvar_relatorio_cache(relatorio_data, relatorio_id):
    """Salva relat√≥rio em cache de arquivo"""
    try:
        cache_file = os.path.join(CACHE_DIR, f'relatorio_{relatorio_id}.pkl')
        with open(cache_file, 'wb') as f:
            pickle.dump({
                'data': relatorio_data,
                'timestamp': datetime.now(),
                'id': relatorio_id
            }, f)
        logger.info(f"Relat√≥rio salvo em cache: {cache_file}")
        return True
    except Exception as e:
        logger.error(f"Erro ao salvar relat√≥rio em cache: {str(e)}")
        return False

def carregar_relatorio_cache(relatorio_id):
    """Carrega relat√≥rio do cache de arquivo"""
    try:
        cache_file = os.path.join(CACHE_DIR, f'relatorio_{relatorio_id}.pkl')
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # Verifica se o cache n√£o √© muito antigo (24 horas)
            if datetime.now() - cache_data['timestamp'] < timedelta(hours=24):
                logger.info(f"Relat√≥rio carregado do cache: {cache_file}")
                return cache_data['data']
            else:
                # Remove cache antigo
                os.unlink(cache_file)
                logger.info(f"Cache antigo removido: {cache_file}")
        return None
    except Exception as e:
        logger.error(f"Erro ao carregar relat√≥rio do cache: {str(e)}")
        return None

def limpar_cache_antigo():
    """Remove caches antigos (mais de 24 horas)"""
    try:
        agora = datetime.now()
        for filename in os.listdir(CACHE_DIR):
            if filename.startswith('relatorio_') and filename.endswith('.pkl'):
                cache_file = os.path.join(CACHE_DIR, filename)
                try:
                    with open(cache_file, 'rb') as f:
                        cache_data = pickle.load(f)
                    if agora - cache_data['timestamp'] > timedelta(hours=24):
                        os.unlink(cache_file)
                        logger.info(f"Cache antigo removido: {filename}")
                except:
                    # Se n√£o conseguir ler, remove o arquivo
                    try:
                        os.unlink(cache_file)
                    except:
                        pass
    except Exception as e:
        logger.error(f"Erro ao limpar cache antigo: {str(e)}")

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configura√ß√µes da API Groq (GRATUITA - Recomendada)
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GROQ_MODEL = os.getenv('GROQ_MODEL', 'llama-3.3-70b-versatile')
MAX_RETRIES = int(os.getenv('MAX_RETRIES', '3'))
RETRY_DELAY = int(os.getenv('RETRY_DELAY', '1'))  # Reduzido para 1 segundo
REQUEST_DELAY = float(os.getenv('REQUEST_DELAY', '0.5'))  # Reduzido para 0.5 segundos para maior velocidade
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '4'))  # N√∫mero de threads paralelas

# Cache para evitar requisi√ß√µes duplicadas
cache_analises = {}

# Knowledge Base da empresa
KNOWLEDGE_BASE_DIR = 'knowledge_base'
knowledge_base_text = ""

def carregar_knowledge_base():
    """Carrega e processa todos os PDFs da pasta knowledge_base"""
    global knowledge_base_text
    knowledge_base_text = ""
    
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        logger.info(f"Pasta knowledge_base n√£o encontrada. Criando pasta...")
        os.makedirs(KNOWLEDGE_BASE_DIR)
        return
    
    pdf_files = [f for f in os.listdir(KNOWLEDGE_BASE_DIR) if f.lower().endswith('.pdf')]
    
    if not pdf_files:
        logger.info("Nenhum PDF encontrado na pasta knowledge_base")
        return
    
    logger.info(f"Carregando {len(pdf_files)} PDFs da knowledge base...")
    
    for pdf_file in pdf_files:
        try:
            pdf_path = os.path.join(KNOWLEDGE_BASE_DIR, pdf_file)
            with open(pdf_path, 'rb') as file:
                pdf_reader = PdfReader(file)
                text_content = ""
                
                for page in pdf_reader.pages:
                    text_content += page.extract_text() + "\n"
                
                knowledge_base_text += f"\n=== CONTE√öDO DO ARQUIVO: {pdf_file} ===\n"
                knowledge_base_text += text_content + "\n"
                
                logger.info(f"PDF carregado: {pdf_file} ({len(pdf_reader.pages)} p√°ginas)")
                
        except Exception as e:
            logger.error(f"Erro ao ler PDF {pdf_file}: {str(e)}")
    
    if knowledge_base_text:
        logger.info(f"Knowledge base carregada com sucesso ({len(knowledge_base_text)} caracteres)")
    else:
        logger.warning("Nenhum conte√∫do p√¥de ser extra√≠do dos PDFs")

if not GROQ_API_KEY:
    logger.error("GROQ_API_KEY n√£o encontrada nas vari√°veis de ambiente")
    raise ValueError("GROQ_API_KEY √© obrigat√≥ria. Configure no arquivo .env")

# Inicializa o cliente Gemini (novo SDK google-genai 1.x)
try:
    gemini_client = genai.Client(api_key=GEMINI_API_KEY)
    logger.info(f"Cliente Gemini configurado com sucesso usando modelo: {GEMINI_MODEL}")
except Exception as e:
    logger.error(f"Erro ao configurar cliente Groq: {str(e)}")
    raise ValueError("N√£o foi poss√≠vel configurar o cliente Groq. Verifique sua API key.")

# Carrega a knowledge base da empresa ao iniciar
carregar_knowledge_base()


# ==================== FUN√á√ïES AUXILIARES ====================

def allowed_file(filename):
    """Verifica se o arquivo tem extens√£o permitida"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def gerar_hash_cache(dados_negocio):
    """Gera um hash √∫nico para os dados do neg√≥cio (evita requisi√ß√µes duplicadas)"""
    dados_str = f"{dados_negocio['negocio']}|{dados_negocio['empresa']}|{dados_negocio['fase']}"
    for i in range(1, 6):
        dados_str += f"|{dados_negocio['historico_descricoes'][f'D{i}']}"
        dados_str += f"|{dados_negocio['historico_temperaturas'][f'F{i}']}"
    return hashlib.md5(dados_str.encode()).hexdigest()


def processar_item_thread(item_data):
    """
    Processa um √∫nico item em thread separada para an√°lise paralela
    """
    try:
        # Identifica follow-ups para exibi√ß√£o
        ultimo_follow, proximo_follow, temperatura_atual = identificar_ultimo_followup(item_data)
        item_data["ultimo_follow"] = ultimo_follow
        item_data["proximo_follow"] = proximo_follow
        item_data["temperatura_atual"] = temperatura_atual
        
        # Chama a IA para an√°lise estrat√©gica
        logger.info(f"Analisando neg√≥cio em paralelo: {item_data['negocio']}")
        item_data["analise_proximo_passo"] = pedir_estrategia_ia(item_data)
        
        return item_data, None  # Sucesso
        
    except Exception as e:
        logger.warning(f"Erro na IA para {item_data['negocio']}: {str(e)}")
        # An√°lise simples baseada nos dados dispon√≠veis
        temp_atual = item_data.get('temperatura_atual', 'N√£o informada')
        fase = item_data.get('fase', 'N√£o informada')
        
        item_data["analise_proximo_passo"] = f"""1. **SITUA√á√ÉO:** Cliente aguardando retorno em {fase}.
2. **A√á√ÉO:** "Ol√° {item_data['empresa']}, passando para confirmar se recebeu minha proposta de {item_data['negocio']}."
3. **META:** Confirmar recebimento e agendar breve alinhamento."""
        
        return item_data, str(e)  # Erro mas com an√°lise fallback


def identificar_ultimo_followup(dados_negocio):
    """
    REGRA DE OURO: Identifica onde a conversa parou.
    Procura do Follow-up 5 para o 1 para encontrar o √∫ltimo preenchido.
    Retorna: (numero_followup, proximo_followup, temperatura_atual)
    """
    ultimo_follow = 0
    temperatura_atual = "N√£o informada"
    
    # Procura do 5 para o 1 (do mais recente para o mais antigo)
    for i in range(5, 0, -1):
        descricao = dados_negocio['historico_descricoes'][f'D{i}'].strip()
        if descricao:  # Se encontrou descri√ß√£o preenchida
            ultimo_follow = i
            temperatura_atual = dados_negocio['historico_temperaturas'][f'F{i}'].strip() or "N√£o informada"
            break
    
    # Se n√£o encontrou nenhum, significa que est√° no in√≠cio (Follow-up 1)
    if ultimo_follow == 0:
        proximo_follow = 1
    elif ultimo_follow < 5:
        proximo_follow = ultimo_follow + 1
    else:
        proximo_follow = 5  # J√° est√° no √∫ltimo
    
    return ultimo_follow, proximo_follow, temperatura_atual


def pedir_estrategia_ia(dados_negocio):
    """
    Envia o contexto do neg√≥cio para a IA Groq e recebe a estrat√©gia de venda.
    A IA age como um Diretor Comercial experiente.
    """
    # Identifica o hash para evitar requisi√ß√µes duplicadas
    hash_cache = gerar_hash_cache(dados_negocio)
    
    # Busca cache desativada para garantir que as novas orienta√ß√µes curtas sejam aplicadas
    # if hash_cache in cache_analises:
    #     return cache_analises[hash_cache]

    # Identifica onde a conversa parou
    ultimo_follow, proximo_follow, temperatura_atual = identificar_ultimo_followup(dados_negocio)
    
    # Monta hist√≥rico relevante (apenas os follow-ups preenchidos)
    historico_texto = ""
    for i in range(1, ultimo_follow + 1):
        desc = dados_negocio['historico_descricoes'][f'D{i}'].strip()
        temp = dados_negocio['historico_temperaturas'][f'F{i}'].strip()
        if desc:
            historico_texto += f"Follow-up {i} (Temperatura: {temp or 'N√£o informada'}): {desc}\n"
    
    # Adiciona conhecimento da empresa se dispon√≠vel
    conhecimento_empresa = ""
    if knowledge_base_text:
        conhecimento_empresa = f"""
CONHECIMENTO DA EMPRESA (VENDAMAIS):
{knowledge_base_text[:10000]}

Use TODO o conte√∫do t√©cnico acima para embasar sua an√°lise. Cite produtos, servi√ßos e metodologias espec√≠ficas da Vendamais."""
    else:
        conhecimento_empresa = "NOTA: Nenhum documento da empresa dispon√≠vel. Use as melhores pr√°ticas mundiais de vendas B2B de alto ticket."
    
    prompt = f"""Voc√™ √© um Diretor Comercial S√™nior da Vendamais. D√™ uma orienta√ß√£o estrat√©gica equilibrada para o vendedor "{dados_negocio['responsavel']}".

{conhecimento_empresa}

CONTEXTO:
- Neg√≥cio/Cliente: {dados_negocio['negocio']} | {dados_negocio['empresa']}
- Pr√≥ximo Passo: Follow-up #{proximo_follow} (Temperatura: {temperatura_atual})

HIST√ìRICO: {historico_texto if historico_texto else 'In√≠cio de prospec√ß√£o.'}

ESTRUTURA DA RESPOSTA:
1. **SITUA√á√ÉO:** Resuma em um par√°grafo curto (3-4 frases) o cen√°rio atual, o que o cliente est√° sentindo e o principal desafio.
2. **MENSAGEM RECOMENDADA:** Crie uma mensagem persuasiva e profissional pronta para enviar. Use gatilhos mentais da Vendamais.
3. **PR√ìXIMO PASSO:** Defina a meta clara deste contato e como conduzir para o fechamento.

REGRA: Seja direto e profissional. Evite introdu√ß√µes desnecess√°rias, mas forne√ßa subst√¢ncia estrat√©gica em cada t√≥pico."""

    logger.info(f"Processando an√°lise ROBUSTA para: {dados_negocio['negocio']} - Pr√≥ximo Follow-up: #{proximo_follow}")

    # Lista de modelos v√°lidos (em ordem de prefer√™ncia)
    modelos_validos = [
        'llama-3.3-70b-versatile',  # Modelo atual recomendado
        'llama-3.1-8b-instruct',   # Fallback r√°pido
        'mixtral-8x7b-32768',       # Alternativa Mixtral
        'gemma2-9b-it'              # Alternativa Gemma
    ]
    
    modelo_usar = GROQ_MODEL if GROQ_MODEL in modelos_validos else modelos_validos[0]
    
    # Tenta at√© o limite configurado caso a API esteja ocupada
    for tentativa in range(MAX_RETRIES):
        try:
            # Delay entre requisi√ß√µes para estabilidade
            if tentativa > 0:
                delay = RETRY_DELAY * (2 ** (tentativa - 1))
                logger.warning(f"Aguardando {delay}s antes da tentativa {tentativa + 1}...")
                time.sleep(delay)

            response = gemini_client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=genai_types.GenerateContentConfig(
                    max_output_tokens=8192,  # AUMENTADO PARA M√ÅXIMA ROBUSTEZ
                    temperature=0.8,         # Criatividade leve para melhores argumentos
                    top_p=0.95,
                    top_k=40
                )
            )
            
            resultado = response.text
            
            # Se a resposta vier vazia ou muito curta, for√ßa um erro para tentar de novo
            if not resultado or len(resultado) < 200:
                raise ValueError("Resposta da IA muito curta ou vazia. Tentando novamente para garantir robustez.")
            
            # Salva no cache para uso futuro (ap√≥s o per√≠odo de testes)
            cache_analises[hash_cache] = resultado
            
            logger.info(f"An√°lise ROBUSTA Gemini gerada com sucesso para {dados_negocio['negocio']}")
            return resultado
            
            # Salva no cache
            cache_analises[hash_cache] = resultado
            
            logger.info(f"An√°lise gerada com sucesso para {dados_negocio['negocio']} usando modelo {modelo_usar} (tentativa {tentativa + 1})")
            return resultado
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # Se o modelo foi descontinuado, tenta outro modelo
            if "decommissioned" in error_msg or "no longer supported" in error_msg or "model_decommissioned" in error_msg:
                logger.warning(f"Modelo {modelo_usar} foi descontinuado. Tentando modelo alternativo...")
                # Tenta pr√≥ximo modelo da lista
                idx_atual = modelos_validos.index(modelo_usar) if modelo_usar in modelos_validos else 0
                if idx_atual < len(modelos_validos) - 1:
                    modelo_usar = modelos_validos[idx_atual + 1]
                    logger.info(f"Tentando com modelo alternativo: {modelo_usar}")
                    continue
                else:
                    logger.error(f"Todos os modelos testados foram descontinuados")
                    return "Erro: Modelo de IA descontinuado. Por favor, atualize o GROQ_MODEL no arquivo .env para 'llama-3.3-70b-versatile'"
            
            if "rate" in error_msg or "limit" in error_msg or "too many" in error_msg:
                logger.warning(f"Limite de cota Groq atingido. Tentativa {tentativa + 1}/{MAX_RETRIES}")
                if tentativa < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY)
                continue
            else:
                logger.error(f"Erro na an√°lise do neg√≥cio {dados_negocio['negocio']}: {str(e)}")
                return f"Erro na an√°lise desta linha: {str(e)}"

    logger.error(f"N√£o foi poss√≠vel gerar an√°lise para {dados_negocio['negocio']} (limite de tentativas excedido)")
    return "N√£o foi poss√≠vel gerar a an√°lise para este item (limite de tentativas excedido)."


def filtrar_negocios_por_fase(relatorio):
    """
    Filtra neg√≥cios para mostrar apenas a partir da Fase Proposta
    Oculta: Oportunidade, Contato, Conectado e Reuni√£o
    Mostra: Proposta, Follow up 1, Follow up 2, Follow up 3, Follow up 4, Follow up 5, Negocia√ß√£o, etc.
    """
    fases_ocultar = ['oportunidade', 'contato', 'conectado', 'reuni√£o']
    
    relatorio_filtrado = []
    for item in relatorio:
        fase_atual = item.get('fase', '').lower().strip()
        
        # Verifica se a fase atual n√£o est√° na lista de fases a ocultar
        # Se n√£o tiver fase ou se for a partir de Proposta, inclui
        if not fase_atual or not any(fase_oculta in fase_atual for fase_oculta in fases_ocultar):
            relatorio_filtrado.append(item)
    
    # LOG DETALHADO para debug
    logger.info(f"=== DEBUG DO FILTRO ===")
    logger.info(f"Fases a ocultar: {fases_ocultar}")
    logger.info(f"Total original: {len(relatorio)} itens")
    
    # Conta por fase
    contagem_fases = {}
    for item in relatorio:
        fase = item.get('fase', 'N√£o informada')
        contagem_fases[fase] = contagem_fases.get(fase, 0) + 1
    
    logger.info(f"Contagem por fase: {contagem_fases}")
    logger.info(f"Total ap√≥s filtro: {len(relatorio_filtrado)} itens")
    
    # Conta por respons√°vel ap√≥s filtro
    contagem_responsaveis = {}
    for item in relatorio_filtrado:
        resp = item.get('responsavel', 'N√£o informado')
        contagem_responsaveis[resp] = contagem_responsaveis.get(resp, 0) + 1
    
    logger.info(f"Respons√°veis ap√≥s filtro: {contagem_responsaveis}")
    logger.info(f"=========================")
    
    return relatorio_filtrado


def normalizar_nome_coluna(nome):
    """Normaliza nome de coluna removendo acentos, espa√ßos extras, aspas e convertendo para min√∫sculas"""
    import unicodedata
    # Remove aspas primeiro
    nome = str(nome).replace('"', '').replace("'", "").strip()
    # Remove acentos
    nome = unicodedata.normalize('NFD', nome)
    nome = ''.join(char for char in nome if unicodedata.category(char) != 'Mn')
    # Remove espa√ßos extras e converte para min√∫sculas
    nome = ' '.join(nome.split()).lower()
    return nome

def encontrar_coluna_similar(df, nome_procurado):
    """Encontra coluna similar no DataFrame (case-insensitive, sem acentos, ignora 'do')"""
    nome_normalizado = normalizar_nome_coluna(nome_procurado)
    
    # Remove palavras comuns que podem variar para compara√ß√£o
    palavras_ignorar = {'do', 'da', 'de', 'o', 'a', 'e', 'up', 'follow', 'proposta', 'da', 'proposta'}
    
    def limpar_palavras(texto):
        palavras = texto.split()
        return set(p for p in palavras if p not in palavras_ignorar)
    
    palavras_procuradas = limpar_palavras(nome_normalizado)
    
    # Primeiro tenta match exato (sem palavras ignoradas)
    for col in df.columns:
        col_normalizada = normalizar_nome_coluna(str(col))
        if nome_normalizado == col_normalizada:
            logger.debug(f"Match exato encontrado: '{col}' -> '{nome_procurado}'")
            return col
    
    # Depois tenta match por palavras importantes
    melhor_match = None
    melhor_score = 0
    
    for col in df.columns:
        col_normalizada = normalizar_nome_coluna(str(col))
        palavras_coluna = limpar_palavras(col_normalizada)
        
        if palavras_procuradas and palavras_coluna:
            # Calcula quantas palavras importantes est√£o presentes
            palavras_comuns = palavras_procuradas.intersection(palavras_coluna)
            if palavras_procuradas:  # Evita divis√£o por zero
                score = len(palavras_comuns) / len(palavras_procuradas)
            else:
                score = 0
            
            # Se encontrou todas as palavras importantes ou pelo menos 60% (reduzido de 70% para ser mais flex√≠vel)
            if score > melhor_score and score >= 0.6:
                melhor_score = score
                melhor_match = col
                logger.debug(f"Match parcial encontrado (score {score:.2f}): '{col}' -> '{nome_procurado}'")
    
    return melhor_match

def normalizar_colunas_df(df):
    """Normaliza nomes das colunas do DataFrame para nomes padr√£o"""
    mapeamento = {}
    
    # LOG para debug das colunas encontradas
    logger.info(f"=== DEBUG COLUNAS ===")
    logger.info(f"Colunas originais: {list(df.columns)}")
    
    # Mapeamento de colunas esperadas para varia√ß√µes poss√≠veis
    colunas_esperadas = {
        'Nome do neg√≥cio': ['nome do negocio', 'nome do neg√≥cio', 'negocio', 'neg√≥cio'],
        'Empresa': ['empresa'],
        'Fase': ['fase'],
        'Responsavel': ['responsavel', 'respons√°vel', 'vendedor', 'usuario', 'usu√°rio', 'usuario', 'usu√°rio'],
        'Temperatura da Proposta Follow 1': ['temperatura da proposta follow 1', 'temperatura follow 1', 'temperatura 1'],
        'Descri√ß√£o Follow up 1': ['descri√ß√£o follow up 1', 'descri√ß√£o do follow up 1', 'descricao follow up 1', 'descricao do follow up 1', 'descri√ß√£o do follow up 1', 'descricao do follow up 1', 'follow up 1'],
        'Temperatura da Proposta Follow 2': ['temperatura da proposta follow 2', 'temperatura follow 2', 'temperatura 2'],
        'Descri√ß√£o Follow up 2': ['descri√ß√£o follow up 2', 'descri√ß√£o do follow up 2', 'descricao follow up 2', 'descricao do follow up 2', 'follow up 2'],
        'Temperatura da Proposta Follow 3': ['temperatura da proposta follow 3', 'temperatura follow 3', 'temperatura 3'],
        'Descri√ß√£o Follow up 3': ['descri√ß√£o follow up 3', 'descri√ß√£o do follow up 3', 'descricao follow up 3', 'descricao do follow up 3', 'follow up 3'],
        'Temperatura da Proposta Follow 4': ['temperatura da proposta follow 4', 'temperatura follow 4', 'temperatura 4'],
        'Descri√ß√£o Follow up 4': ['descri√ß√£o follow up 4', 'descri√ß√£o do follow up 4', 'descricao follow up 4', 'descricao do follow up 4', 'follow up 4'],
        'Temperatura da Proposta Follow 5': ['temperatura da proposta follow 5', 'temperatura follow 5', 'temperatura 5'],
        'Descri√ß√£o Follow up 5': ['descri√ß√£o follow up 5', 'descri√ß√£o do follow up 5', 'descricao follow up 5', 'descricao do follow up 5', 'follow up 5'],
    }
    
    # Para cada coluna esperada, tenta encontrar no DataFrame
    for coluna_esperada, variacoes in colunas_esperadas.items():
        coluna_encontrada = encontrar_coluna_similar(df, coluna_esperada)
        if coluna_encontrada:
            mapeamento[coluna_encontrada] = coluna_esperada
        else:
            # Tenta com varia√ß√µes
            for variacao in variacoes:
                coluna_encontrada = encontrar_coluna_similar(df, variacao)
                if coluna_encontrada:
                    mapeamento[coluna_encontrada] = coluna_esperada
                    break
    
    # Renomeia as colunas encontradas
    if mapeamento:
        df = df.rename(columns=mapeamento)
        logger.info(f"Colunas normalizadas ({len(mapeamento)} colunas): {list(mapeamento.items())}")
    else:
        logger.warning("Nenhuma coluna foi normalizada. Verifique se os nomes das colunas est√£o corretos.")
    
    # Se for CSV sem cabe√ßalho, mapeia por posi√ß√£o
    if df.columns.tolist() == list(range(df.shape[1])):
        logger.info("CSV sem cabe√ßalho detectado, mapeando por posi√ß√£o...")
        mapeamento_posicional = {
            0: 'Empresa',
            2: 'Fase', 
            3: 'Responsavel',
            7: 'Descri√ß√£o Follow up 1',
            8: 'Descri√ß√£o Follow up 2', 
            9: 'Descri√ß√£o Follow up 3',
            10: 'Descri√ß√£o Follow up 4',
            11: 'Descri√ß√£o Follow up 5',
            12: 'Temperatura da Proposta Follow 1',
            13: 'Temperatura da Proposta Follow 2',
            14: 'Temperatura da Proposta Follow 3', 
            15: 'Temperatura da Proposta Follow 4',
            16: 'Temperatura da Proposta Follow 5',
        }
        
        for pos, nome in mapeamento_posicional.items():
            if pos < df.shape[1]:
                df.rename(columns={pos: nome}, inplace=True)
        
        logger.info(f"Mapeamento posicional aplicado: {mapeamento_posicional}")
    
    # Cria colunas faltantes com valores vazios (para garantir que o sistema funcione)
    colunas_esperadas = [
        'Nome do neg√≥cio', 'Empresa', 'Fase', 'Responsavel',
        'Temperatura da Proposta Follow 1', 'Descri√ß√£o Follow up 1',
        'Temperatura da Proposta Follow 2', 'Descri√ß√£o Follow up 2',
        'Temperatura da Proposta Follow 3', 'Descri√ß√£o Follow up 3',
        'Temperatura da Proposta Follow 4', 'Descri√ß√£o Follow up 4',
        'Temperatura da Proposta Follow 5', 'Descri√ß√£o Follow up 5',
    ]
    
    colunas_criadas = []
    for coluna in colunas_esperadas:
        if coluna not in df.columns:
            df[coluna] = ''  # Cria coluna vazia
            colunas_criadas.append(coluna)
    
    if colunas_criadas:
        logger.info(f"Colunas criadas automaticamente (vazias): {', '.join(colunas_criadas)}")
    
    logger.info(f"Colunas finais: {list(df.columns)}")
    logger.info(f"=====================")
    
    return df

def validar_planilha(df):
    """
    Valida a planilha de forma flex√≠vel - apenas informa colunas faltantes, mas NUNCA bloqueia.
    Esta fun√ß√£o sempre retorna True e nunca gera exce√ß√µes.
    """
    try:
        colunas_desejadas = [
            'Nome do neg√≥cio', 'Empresa', 'Fase', 'Responsavel',
            'Temperatura da Proposta Follow 1', 'Descri√ß√£o Follow up 1'
        ]

        colunas_faltantes = []
        colunas_encontradas = []
        
        for coluna in colunas_desejadas:
            if coluna in df.columns:
                colunas_encontradas.append(coluna)
            else:
                colunas_faltantes.append(coluna)

        if colunas_encontradas:
            logger.info(f"‚úÖ Colunas encontradas: {', '.join(colunas_encontradas)}")
        
        if colunas_faltantes:
            logger.warning(f"‚ö†Ô∏è Colunas n√£o encontradas (sistema continuar√° funcionando normalmente): {', '.join(colunas_faltantes)}")
            logger.info(f"üìã Todas as colunas dispon√≠veis no arquivo: {', '.join(list(df.columns)[:20])}")

        # SEMPRE retorna True - nunca bloqueia
        return True
    except Exception as e:
        # Se der qualquer erro, apenas loga e continua
        logger.warning(f"Erro na valida√ß√£o (mas continuando): {str(e)}")
        return True  # Sempre retorna True para n√£o bloquear


def ler_planilha_excel(file_path, filename):
    """
    L√™ arquivo Excel/CSV com m√∫ltiplas estrat√©gias de fallback.
    Suporta .xlsx, .xls, .csv e at√© arquivos HTML disfar√ßados de Excel.
    """
    file_ext = filename.lower().split('.')[-1] if '.' in filename else ''
    logger.info(f"Processando arquivo.{file_ext}: {filename}")
    
    df = None
    error_messages = []
    
    # PRIORIDADE 0: Se for CSV, l√™ diretamente (mais simples e confi√°vel)
    if file_ext == 'csv':
        logger.info("Arquivo CSV detectado, lendo diretamente...")
        try:
            # Tenta diferentes separadores e encodings comuns
            separadores = [';', ',', '\t']
            encodings = ['utf-8-sig', 'utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
            
            for encoding in encodings:
                for sep in separadores:
                    try:
                        # Primeiro tenta ler com cabe√ßalho (header='infer')
                        df = pd.read_csv(file_path, sep=sep, encoding=encoding, skipinitialspace=True)
                        if len(df.columns) > 1:
                            logger.info(f"‚úÖ CSV lido com sucesso (separador='{sep}', encoding={encoding}): {len(df)} linhas, {len(df.columns)} colunas")
                            break
                    except Exception:
                        continue
                if df is not None and len(df.columns) > 1:
                    break
                # Se ainda n√£o tem cabe√ßalho reconhec√≠vel, tenta ler sem cabe√ßalho
                try:
                    df = pd.read_csv(file_path, sep=sep, encoding=encoding, header=None, skipinitialspace=True)
                    logger.info(f"‚úÖ CSV lido sem cabe√ßalho (separador='{sep}', encoding={encoding}): {len(df)} linhas, {len(df.columns)} colunas")
                except Exception:
                    df = None
            
            # Se ainda n√£o conseguiu, tenta sem especificar separador (detec√ß√£o autom√°tica)
            if df is None or len(df.columns) <= 1:
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding, skipinitialspace=True)
                        if len(df.columns) > 1:
                            logger.info(f"‚úÖ CSV lido com detec√ß√£o autom√°tica (encoding={encoding}): {len(df)} linhas, {len(df.columns)} colunas")
                            break
                    except Exception as e:
                        continue
            
            if df is None or len(df.columns) <= 1:
                error_messages.append("N√£o foi poss√≠vel ler o CSV com nenhum separador/encoding testado")
        except Exception as e:
            error_messages.append(f"Erro ao ler CSV: {str(e)}")
        
        if df is not None and not df.empty:
            # Se o CSV n√£o tem cabe√ßalho, atribu√≠mos nomes de colunas esperados com base na posi√ß√£o conhecida
            if df.columns.tolist() == list(range(df.shape[1])):
                # Mapeamento posicional (ajuste conforme seu CSV)
                colunas_pos = [
                    'Empresa',          # 0
                    'Tipo',            # 1 (ignorado)
                    'Fase',            # 2 (ignorado)
                    'Responsavel',     # 3
                    'Data',            # 4 (ignorado)
                    # ... campos intermedi√°rios ignorados ...
                    'Temperatura Atual',  # pen√∫ltimo antes do ID, ajuste conforme necessidade
                ]
                # Preencher at√© o n√∫mero de colunas existentes
                for i, nome in enumerate(colunas_pos):
                    if i < df.shape[1]:
                        df.rename(columns={i: nome}, inplace=True)
                logger.info("Colunas do CSV sem cabe√ßalho foram renomeadas com base em posi√ß√µes conhecidas.")
            return df
    
    # PRIMEIRO: Verifica assinaturas de arquivo Excel v√°lido
    is_valid_excel = False
    is_html = False
    
    try:
        with open(file_path, 'rb') as f:
            header = f.read(8)  # L√™ apenas os primeiros 8 bytes para verificar assinatura
            
            # Assinaturas de arquivos Excel v√°lidos
            excel_signatures = [
                b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1',  # .xls (OLE2 format)
                b'\x50\x4b\x03\x04',  # .xlsx (ZIP format - come√ßa com PK)
                b'\x50\x4b\x05\x06',  # .xlsx (ZIP empty)
                b'\x50\x4b\x07\x08'   # .xlsx (ZIP spanned)
            ]
            
            # Verifica se √© um Excel v√°lido
            for sig in excel_signatures:
                if header.startswith(sig):
                    is_valid_excel = True
                    logger.info(f"Assinatura Excel v√°lida detectada: {sig.hex()}")
                    break
            
            # Se n√£o √© Excel v√°lido, verifica se √© HTML (l√™ mais bytes)
            if not is_valid_excel:
                f.seek(0)
                header_full = f.read(500)
                
                # Detecta HTML de v√°rias formas (incluindo BOM)
                # O caso mais comum: arquivo HTML salvo com extens√£o .xls
                is_html = (
                    header_full.startswith(b'\xef\xbb\xbf<meta') or  # BOM + <meta
                    header_full.startswith(b'<meta') or 
                    header_full.startswith(b'<!DOCTYPE') or 
                    header_full.startswith(b'<html') or
                    header_full.startswith(b'\xef\xbb\xbf<!DOCTYPE') or
                    header_full.startswith(b'\xef\xbb\xbf<html') or
                    (b'<table' in header_full and b'<tr>' in header_full and b'<td>' in header_full) or
                    (b'http-equiv' in header_full and b'Content-type' in header_full)  # Meta tag comum em HTML
                )
                
                if is_html:
                    logger.warning("HTML detectado no arquivo (arquivo HTML salvo com extens√£o .xls/.xlsx)")
    except Exception as e:
        logger.warning(f"Erro ao verificar header do arquivo: {str(e)}")
    
    # PRIORIDADE 1: Tenta ler como Excel primeiro (APENAS se tem assinatura v√°lida E n√£o √© HTML)
    if is_valid_excel and not is_html:
        logger.info("Tentando ler como arquivo Excel v√°lido...")
        
        if file_ext == 'xls':
            # Para .xls, tenta xlrd primeiro (mais compat√≠vel)
            try:
                df = pd.read_excel(file_path, engine='xlrd')
                logger.info("‚úÖ Arquivo .xls lido com sucesso usando xlrd")
            except Exception as e1:
                logger.warning(f"xlrd falhou: {str(e1)}")
                error_messages.append(f"xlrd: {str(e1)}")
                
                # Tenta openpyxl como fallback
                try:
                    df = pd.read_excel(file_path, engine='openpyxl')
                    logger.info("‚úÖ Arquivo .xls lido com sucesso usando openpyxl (fallback)")
                except Exception as e2:
                    logger.warning(f"openpyxl tamb√©m falhou: {str(e2)}")
                    error_messages.append(f"openpyxl: {str(e2)}")
                
                # Tenta sem engine espec√≠fica
                if df is None:
                    try:
                        df = pd.read_excel(file_path)
                        logger.info("‚úÖ Arquivo .xls lido sem engine espec√≠fica")
                    except Exception as e3:
                        error_messages.append(f"default: {str(e3)}")
        
        elif file_ext == 'xlsx':
            # Para .xlsx, tenta openpyxl primeiro
            try:
                df = pd.read_excel(file_path, engine='openpyxl')
                logger.info("‚úÖ Arquivo .xlsx lido com sucesso usando openpyxl")
            except Exception as e1:
                logger.warning(f"openpyxl falhou: {str(e1)}")
                error_messages.append(f"openpyxl: {str(e1)}")
                
                # Tenta xlrd como fallback
                try:
                    df = pd.read_excel(file_path, engine='xlrd')
                    logger.info("‚úÖ Arquivo .xlsx lido com sucesso usando xlrd (fallback)")
                except Exception as e2:
                    logger.warning(f"xlrd tamb√©m falhou: {str(e2)}")
                    error_messages.append(f"xlrd: {str(e2)}")
                
                # Tenta sem engine espec√≠fica
                if df is None:
                    try:
                        df = pd.read_excel(file_path)
                        logger.info("‚úÖ Arquivo .xlsx lido sem engine espec√≠fica")
                    except Exception as e3:
                        error_messages.append(f"default: {str(e3)}")
        
        # Se ainda n√£o conseguiu e tem assinatura Excel, tenta tratamento especial
        if df is None and is_valid_excel:
            logger.warning("Arquivo tem assinatura Excel mas n√£o foi poss√≠vel ler. Tentando tratamento especial...")
            # Tenta remover BOM se existir e ler novamente
            try:
                with open(file_path, 'rb') as f:
                    content = f.read()
                
                # Remove BOM se existir no in√≠cio
                if content.startswith(b'\xef\xbb\xbf'):
                    logger.info("Removendo BOM do arquivo...")
                    content = content[3:]
                    temp_path = file_path + '_no_bom.xls'
                    with open(temp_path, 'wb') as f:
                        f.write(content)
                    
                    try:
                        df = pd.read_excel(temp_path, engine='xlrd')
                        logger.info("‚úÖ Arquivo lido ap√≥s remover BOM")
                    except:
                        pass
                    finally:
                        try:
                            os.unlink(temp_path)
                        except:
                            pass
            except Exception as e:
                logger.warning(f"Tratamento especial falhou: {str(e)}")
    
    # PRIORIDADE 2: Se detectou HTML (mesmo que tenha extens√£o .xls/.xlsx), tenta converter HTML PRIMEIRO
    if is_html:
        logger.warning("Conte√∫do HTML detectado, tentando converter HTML para DataFrame...")
        
        # Estrat√©gia 1: Remove BOM primeiro e tenta pd.read_html
        try:
            with open(file_path, 'rb') as f:
                content_bytes = f.read()
            
            # Remove BOM se existir
            if content_bytes.startswith(b'\xef\xbb\xbf'):
                logger.info("Removendo BOM do arquivo HTML...")
                content_bytes = content_bytes[3:]
            
            # Salva temporariamente sem BOM
            temp_html_path = file_path + '_temp_clean.html'
            with open(temp_html_path, 'wb') as f:
                f.write(content_bytes)
            
            # Tenta ler HTML com diferentes encodings
            encodings_to_try = ['utf-8', 'utf-8-sig', 'latin-1', 'iso-8859-1', 'cp1252']
            for encoding in encodings_to_try:
                try:
                    df_html = pd.read_html(temp_html_path, encoding=encoding)
                    if df_html and len(df_html) > 0:
                        # Pega a primeira tabela com mais colunas (geralmente √© a principal)
                        df = max(df_html, key=lambda x: len(x.columns) if not x.empty else 0)
                        if not df.empty:
                            logger.info(f"‚úÖ HTML convertido com sucesso (encoding={encoding}): {len(df)} linhas, {len(df.columns)} colunas")
                            break
                except Exception as e1:
                    if encoding == encodings_to_try[0]:
                        logger.warning(f"pd.read_html com encoding {encoding} falhou: {str(e1)}")
                        error_messages.append(f"read_html({str(e1)})")
                    continue
            
            # Remove arquivo tempor√°rio
            try:
                os.unlink(temp_html_path)
            except:
                pass
                
        except Exception as e:
            logger.warning(f"Erro ao processar HTML: {str(e)}")
            error_messages.append(f"process_html: {str(e)}")
        
        # Estrat√©gia 2: Se ainda n√£o conseguiu, tenta direto no arquivo original
        if df is None:
            encodings_to_try = ['utf-8', 'utf-8-sig', 'latin-1', 'iso-8859-1', 'cp1252']
            for encoding in encodings_to_try:
                try:
                    df_html = pd.read_html(file_path, encoding=encoding)
                    if df_html and len(df_html) > 0:
                        df = max(df_html, key=lambda x: len(x.columns) if not x.empty else 0)
                        if not df.empty:
                            logger.info(f"‚úÖ HTML convertido diretamente (encoding={encoding}): {len(df)} linhas, {len(df.columns)} colunas")
                            break
                except Exception as e1:
                    continue
        
        # Estrat√©gia 2: Remove BOM manualmente e tenta novamente
        if df is None:
            try:
                with open(file_path, 'rb') as f:
                    content_bytes = f.read()
                
                # Remove BOM se existir
                if content_bytes.startswith(b'\xef\xbb\xbf'):
                    content_bytes = content_bytes[3:]
                
                # Salva temporariamente sem BOM
                temp_html_path = file_path + '_clean.html'
                with open(temp_html_path, 'wb') as f:
                    f.write(content_bytes)
                
                df_html = pd.read_html(temp_html_path, encoding='utf-8')
                if df_html and len(df_html) > 0:
                    df = max(df_html, key=lambda x: len(x.columns) if not x.empty else 0)
                    if not df.empty:
                        logger.info(f"HTML convertido ap√≥s remover BOM: {len(df)} linhas, {len(df.columns)} colunas")
                
                # Remove arquivo tempor√°rio
                try:
                    os.unlink(temp_html_path)
                except:
                    pass
            except Exception as e2:
                logger.warning(f"Convers√£o HTML com BOM removido falhou: {str(e2)}")
                error_messages.append(f"read_html_bom({str(e2)})")
        
        # Estrat√©gia 3: Tenta ler como CSV (√†s vezes HTML √© salvo como CSV)
        if df is None:
            try:
                for sep in [';', ',', '\t']:
                    try:
                        df_test = pd.read_csv(file_path, sep=sep, encoding='utf-8-sig', skiprows=0)
                        if len(df_test.columns) > 1:  # Se encontrou m√∫ltiplas colunas
                            df = df_test
                            logger.info(f"HTML lido como CSV com separador '{sep}': {len(df)} linhas, {len(df.columns)} colunas")
                            break
                    except:
                        continue
            except Exception as e4:
                logger.warning(f"Leitura como CSV falhou: {str(e4)}")
    
    # PRIORIDADE 3: Se n√£o √© HTML e tem extens√£o .xls/.xlsx mas n√£o tem assinatura v√°lida, tenta ler como Excel
    if df is None and not is_html and file_ext in ['xls', 'xlsx']:
        logger.info("Tentando ler como Excel (extens√£o .xls/.xlsx mas sem assinatura detectada)...")
        if file_ext == 'xls':
            try:
                df = pd.read_excel(file_path, engine='xlrd')
                logger.info("‚úÖ Arquivo .xls lido com sucesso usando xlrd")
            except Exception as e1:
                logger.warning(f"xlrd falhou: {str(e1)}")
                error_messages.append(f"xlrd: {str(e1)}")
                try:
                    df = pd.read_excel(file_path, engine='openpyxl')
                    logger.info("‚úÖ Arquivo .xls lido com sucesso usando openpyxl")
                except Exception as e2:
                    error_messages.append(f"openpyxl: {str(e2)}")
        elif file_ext == 'xlsx':
            try:
                df = pd.read_excel(file_path, engine='openpyxl')
                logger.info("‚úÖ Arquivo .xlsx lido com sucesso usando openpyxl")
            except Exception as e1:
                logger.warning(f"openpyxl falhou: {str(e1)}")
                error_messages.append(f"openpyxl: {str(e1)}")
                try:
                    df = pd.read_excel(file_path, engine='xlrd')
                    logger.info("‚úÖ Arquivo .xlsx lido com sucesso usando xlrd")
                except Exception as e2:
                    error_messages.append(f"xlrd: {str(e2)}")
    
    # Se ainda n√£o conseguiu ler, d√° mensagem de erro clara
    if df is None:
        if is_html:
            raise ValueError(
                "O arquivo parece ser HTML (p√°gina web) e n√£o um arquivo Excel v√°lido. "
                "Por favor, abra o arquivo no Excel e salve como '.xlsx' ou '.xls' antes de enviar. "
                f"Erros de convers√£o: {', '.join(error_messages) if error_messages else 'N√£o foi poss√≠vel converter HTML'}"
            )
        elif is_valid_excel:
            all_errors = ", ".join(error_messages) if error_messages else "Erro desconhecido"
            raise ValueError(
                f"O arquivo tem assinatura Excel v√°lida mas n√£o foi poss√≠vel ler. "
                f"O arquivo pode estar corrompido. Erros: {all_errors}. "
                f"Tente abrir o arquivo no Excel e salvar novamente."
            )
        else:
            all_errors = ", ".join(error_messages) if error_messages else "Erro desconhecido"
            raise ValueError(
                f"N√£o foi poss√≠vel ler o arquivo Excel. "
                f"Verifique se o arquivo n√£o est√° corrompido. Erros: {all_errors}. "
                f"Se o problema persistir, tente abrir o arquivo no Excel e salvar novamente como .xlsx"
            )
    
    if df.empty:
        raise ValueError("O arquivo est√° vazio ou n√£o cont√©m dados v√°lidos")
    
    return df


# ==================== ROTAS ====================

@app.route('/')
def index():
    """P√°gina inicial com formul√°rio de upload"""
    return render_template('index.html')


@app.route('/todos')
def ver_todos():
    """Visualiza√ß√£o de todos os neg√≥cios filtrados por fase (Proposta para frente)"""
    try:
        logger.info("=== ROTA /TOS INICIADA ===")
        
        # Carrega dados do cache
        relatorio_final = None
        relatorio_agrupado = None
        
        # Tenta 1: Cache de arquivo
        if 'relatorio_id_atual' in session:
            relatorio_id = session['relatorio_id_atual']
            logger.info(f"Found relatorio_id_atual in session: {relatorio_id}")
            dados_cache = carregar_relatorio_cache(relatorio_id)
            if dados_cache:
                if isinstance(dados_cache, dict):
                    relatorio_final = dados_cache.get('relatorio_final', [])
                    relatorio_agrupado = dados_cache.get('relatorio_agrupado', {})
                    logger.info(f"Cache loaded: {len(relatorio_final)} itens, {len(relatorio_agrupado)} respons√°veis")
                else:
                    # Formato antigo - converte para novo
                    relatorio_final = dados_cache
                    relatorio_agrupado = {}
                    for item in relatorio_final:
                        responsavel_item = item.get('responsavel', 'N√£o informado')
                        if responsavel_item not in relatorio_agrupado:
                            relatorio_agrupado[responsavel_item] = []
                        relatorio_agrupado[responsavel_item].append(item)
                    logger.info(f"Old format converted: {len(relatorio_agrupado)} respons√°veis")
            else:
                logger.warning(f"Failed to load cache for ID: {relatorio_id}")
        else:
            logger.warning("relatorio_id_atual not found in session")
        
        # Tenta 2: Sess√£o (backup)
        if not relatorio_agrupado and 'relatorio_data' in session:
            logger.info("Trying backup session data")
            dados_session = session['relatorio_data']
            if isinstance(dados_session, dict):
                relatorio_final = dados_session.get('relatorio_final', [])
                relatorio_agrupado = dados_session.get('relatorio_agrupado', {})
                logger.info(f"Session backup loaded: {len(relatorio_agrupado)} respons√°veis")
            else:
                relatorio_final = dados_session
                relatorio_agrupado = {}
                for item in relatorio_final:
                    responsavel_item = item.get('responsavel', 'N√£o informado')
                    if responsavel_item not in relatorio_agrupado:
                        relatorio_agrupado[responsavel_item] = []
                    relatorio_agrupado[responsavel_item].append(item)
                logger.info(f"Session backup converted: {len(relatorio_agrupado)} respons√°veis")
        
        # Tenta 3: Busca autom√°tica no cache
        if not relatorio_agrupado:
            logger.info("Trying auto-discovery in cache files")
            try:
                for filename in os.listdir(CACHE_DIR):
                    if filename.startswith('relatorio_') and filename.endswith('.pkl'):
                        cache_file = os.path.join(CACHE_DIR, filename)
                        try:
                            with open(cache_file, 'rb') as f:
                                cache_data = pickle.load(f)
                            
                            if datetime.now() - cache_data['timestamp'] < timedelta(hours=24):
                                dados_cache = cache_data['data']
                                if isinstance(dados_cache, dict) and 'relatorio_agrupado' in dados_cache:
                                    relatorio_agrupado = dados_cache['relatorio_agrupado']
                                    relatorio_final = dados_cache.get('relatorio_final', [])
                                    logger.info(f"Auto-discovered: {len(relatorio_agrupado)} respons√°veis")
                                    break
                        except Exception as e:
                            logger.warning(f"Failed to read cache file {filename}: {e}")
                            continue
            except Exception as e:
                logger.error(f"Error in auto-discovery: {e}")
        
        # Debug final
        if relatorio_agrupado:
            logger.info(f"Respons√°veis encontrados: {list(relatorio_agrupado.keys())}")
            for resp, itens in relatorio_agrupado.items():
                logger.info(f"  - {resp}: {len(itens)} itens")
        else:
            logger.error("Nenhum dado encontrado em nenhum lugar!")
        
        if not relatorio_final:
            flash('Dados n√£o encontrados. Por favor, processe a planilha novamente.', 'error')
            return redirect(url_for('index'))
        
        logger.info(f"=== RENDERIZANDO TEMPLATES COM {len(relatorio_agrupado)} RESPONS√ÅVEIS ===")
        
        return render_template('relatorio.html', 
                             relatorio=relatorio_final, 
                             total=len(relatorio_final),
                             relatorio_agrupado=relatorio_agrupado, 
                             responsaveis=list(relatorio_agrupado.keys()))
        
    except Exception as e:
        logger.error(f"Erro ao visualizar todos os neg√≥cios: {str(e)}")
        flash(f'Erro ao carregar dados: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/processar', methods=['POST'])
def processar():
    """Processa a planilha enviada e gera an√°lises estrat√©gicas"""
    if 'file' not in request.files:
        logger.error("Nenhum arquivo enviado na requisi√ß√£o")
        flash('Nenhum arquivo selecionado', 'error')
        return redirect(url_for('index'))
    
    file = request.files['file']
    
    if file.filename == '':
        logger.error("Nome de arquivo vazio")
        flash('Nenhum arquivo selecionado', 'error')
        return redirect(url_for('index'))
    
    if not allowed_file(file.filename):
        logger.error(f"Formato de arquivo inv√°lido: {file.filename}")
        flash('Formato de arquivo inv√°lido. Envie arquivos Excel (.xlsx, .xls) ou CSV (.csv)', 'error')
        return redirect(url_for('index'))
    
    try:
        logger.info(f"Processando arquivo: {file.filename}")
        
        # Salva temporariamente
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_file:
            file.save(tmp_file.name)
            tmp_file_path = tmp_file.name
        
        try:
            # L√™ a planilha
            df = ler_planilha_excel(tmp_file_path, file.filename)
            
            # Limpeza dos dados
            df = df.fillna("")
            # Remove aspas e espa√ßos extras dos nomes das colunas
            df.columns = df.columns.str.strip().str.replace('"', '').str.replace("'", "")
            
            logger.info(f"Arquivo lido: {len(df)} linhas, {len(df.columns)} colunas")
            logger.info(f"Colunas encontradas (ap√≥s limpeza): {list(df.columns)[:15]}")
            
            # Normaliza nomes das colunas (aceita varia√ß√µes como acentos, "do", etc)
            df = normalizar_colunas_df(df)
            
            logger.info(f"Colunas ap√≥s normaliza√ß√£o: {list(df.columns)[:15]}")
            
            # Valida estrutura (apenas informa, n√£o bloqueia - NUNCA bloqueia)
            try:
                validar_planilha(df)
            except Exception as e:
                # Se por algum motivo der erro na valida√ß√£o, apenas loga e continua
                logger.warning(f"Valida√ß√£o retornou erro (mas continuando): {str(e)}")
            
            # Verifica se tem pelo menos algumas colunas b√°sicas
            colunas_basicas = ['Nome do neg√≥cio', 'Empresa', 'Fase', 'Responsavel']
            tem_colunas_basicas = any(col in df.columns for col in colunas_basicas)
            
            if not tem_colunas_basicas:
                logger.warning("Nenhuma coluna b√°sica encontrada, mas continuando processamento...")
                flash('Aviso: Algumas colunas esperadas n√£o foram encontradas. O sistema continuar√° processando com os dados dispon√≠veis.', 'warning')
            
            # Agrupa dados por Respons√°vel
            relatorio_agrupado = {}
            relatorio_final = []
            linhas_processadas = 0
            linhas_com_erro = 0

            # Prepara todos os itens para processamento
            itens_para_processar = []
            
            for index, linha in df.iterrows():
                try:
                    # Monta o dicion√°rio de dados da linha (usa valores padr√£o se coluna n√£o existir)
                    # Busca colunas de forma flex√≠vel
                    def buscar_coluna(coluna_principal, alternativas=None):
                        """Busca coluna no DataFrame, tentando varia√ß√µes e, se necess√°rio, posi√ß√µes conhecidas."""
                        # Tenta coluna principal
                        if coluna_principal in df.columns:
                            valor = linha.get(coluna_principal, '')
                            if pd.notna(valor):
                                return str(valor).strip()
                        # Tenta alternativas
                        if alternativas:
                            for alt in alternativas:
                                if alt in df.columns:
                                    valor = linha.get(alt, '')
                                    if pd.notna(valor):
                                        return str(valor).strip()
                        # Fallback: tenta usar posi√ß√£o baseada em nomes conhecidos
                        pos_map = {
                            'Empresa': 0,
                            'Responsavel': 3,
                            'Temperatura da Proposta Follow 1': -2,  # pen√∫ltimo campo antes do ID (ajuste conforme CSV)
                        }
                        if coluna_principal in pos_map:
                            idx = pos_map[coluna_principal]
                            if isinstance(idx, int) and abs(idx) < len(linha):
                                valor = linha.iloc[idx] if hasattr(linha, 'iloc') else linha[idx]
                                if pd.notna(valor):
                                    return str(valor).strip()
                        return ''
                    
                    item = {
                        "negocio": buscar_coluna('Nome do neg√≥cio', ['Nome do negocio', 'Neg√≥cio', 'Negocio']) or f'Neg√≥cio {index + 1}',
                        "fase": buscar_coluna('Fase') or 'N√£o informada',
                        "responsavel": buscar_coluna('Responsavel', ['Respons√°vel', 'Vendedor', 'Usuario', 'Usu√°rio']) or 'N√£o informado',
                        "empresa": buscar_coluna('Empresa') or 'N√£o informada',
                        "historico_temperaturas": {
                            "F1": buscar_coluna('Temperatura da Proposta Follow 1', ['Temperatura Follow 1', 'Temperatura 1']),
                            "F2": buscar_coluna('Temperatura da Proposta Follow 2', ['Temperatura Follow 2', 'Temperatura 2']),
                            "F3": buscar_coluna('Temperatura da Proposta Follow 3', ['Temperatura Follow 3', 'Temperatura 3']),
                            "F4": buscar_coluna('Temperatura da Proposta Follow 4', ['Temperatura Follow 4', 'Temperatura 4']),
                            "F5": buscar_coluna('Temperatura da Proposta Follow 5', ['Temperatura Follow 5', 'Temperatura 5']),
                        },
                        "historico_descricoes": {
                            "D1": buscar_coluna('Descri√ß√£o Follow up 1', ['Descri√ß√£o do Follow up 1', 'Descricao Follow up 1', 'Follow up 1']),
                            "D2": buscar_coluna('Descri√ß√£o Follow up 2', ['Descri√ß√£o do Follow up 2', 'Descricao Follow up 2', 'Follow up 2']),
                            "D3": buscar_coluna('Descri√ß√£o Follow up 3', ['Descri√ß√£o do Follow up 3', 'Descricao Follow up 3', 'Follow up 3']),
                            "D4": buscar_coluna('Descri√ß√£o Follow up 4', ['Descri√ß√£o do Follow up 4', 'Descricao Follow up 4', 'Follow up 4']),
                            "D5": buscar_coluna('Descri√ß√£o Follow up 5', ['Descri√ß√£o do Follow up 5', 'Descricao Follow up 5', 'Follow up 5']),
                        }
                    }
                    
                    # Pula linhas completamente vazias (apenas se n√£o tiver nome do neg√≥cio E empresa)
                    if (not item['negocio'] or item['negocio'] == f'Neg√≥cio {index + 1}') and \
                       (not item['empresa'] or item['empresa'] == 'N√£o informada'):
                        logger.info(f"Pulando linha {index + 1} - sem dados b√°sicos (neg√≥cio/empresa)")
                        continue
                    
                    itens_para_processar.append(item)
                        
                except Exception as e:
                    logger.error(f"Erro ao preparar linha {index + 1}: {str(e)}")
                    linhas_com_erro += 1
                    continue

            logger.info(f"Iniciando processamento sequencial de {len(itens_para_processar)} itens (Qualidade > Velocidade)")
            
            # Processamento sequencial (um por um) para maior robustez
            for index, item in enumerate(itens_para_processar):
                try:
                    logger.info(f"Processando item {index + 1}/{len(itens_para_processar)}: {item['negocio']}")
                    
                    # Delay entre requisi√ß√µes para evitar burst e garantir estabilidade
                    if index > 0:
                        time.sleep(REQUEST_DELAY)
                    
                    item_processado, erro = processar_item_thread(item)
                    
                    if erro:
                        logger.warning(f"Item {index + 1} processado com erro: {erro}")
                        linhas_com_erro += 1
                    else:
                        linhas_processadas += 1
                        logger.info(f"‚úÖ Item {index + 1} conclu√≠do com sucesso")
                    
                    relatorio_final.append(item_processado)
                    
                    # Agrupamento por Respons√°vel
                    responsavel = item_processado.get('responsavel') or 'N√£o informado'
                    if responsavel not in relatorio_agrupado:
                        relatorio_agrupado[responsavel] = []
                    relatorio_agrupado[responsavel].append(item_processado)
                    
                except Exception as e:
                    logger.error(f"Erro ao processar item {index + 1}: {str(e)}")
                    linhas_com_erro += 1

            logger.info(f"Processamento conclu√≠do: {linhas_processadas} sucessos, {linhas_com_erro} erros")
            logger.info(f"Respons√°veis identificados: {list(relatorio_agrupado.keys())}")
            
            if linhas_processadas == 0:
                flash('Nenhuma linha v√°lida encontrada na planilha', 'warning')
                return redirect(url_for('index'))
            
            # Aplica filtro de fase - mostra apenas a partir da Fase Proposta
            relatorio_final_filtrado = filtrar_negocios_por_fase(relatorio_final)
            
            # Reagrupa os dados filtrados por respons√°vel
            relatorio_agrupado_filtrado = {}
            for item in relatorio_final_filtrado:
                responsavel = item.get('responsavel', 'N√£o informado')
                if responsavel not in relatorio_agrupado_filtrado:
                    relatorio_agrupado_filtrado[responsavel] = []
                relatorio_agrupado_filtrado[responsavel].append(item)
            
            # Armazena na sess√£o e no cache
            import uuid
            relatorio_id = str(uuid.uuid4())[:8]
            
            # Salva no cache de arquivo (agora com dados filtrados)
            dados_cache = {
                'relatorio_final': relatorio_final_filtrado,
                'relatorio_agrupado': relatorio_agrupado_filtrado,
                'responsaveis': list(relatorio_agrupado_filtrado.keys())
            }
            salvar_relatorio_cache(dados_cache, relatorio_id)
            
            # ATEN√á√ÉO: Salvamos APENAS o ID na sess√£o para n√£o estourar o limite de cookie
            session['relatorio_id_atual'] = relatorio_id
            session.permanent = True 
            
            # Removemos dados pesados da sess√£o que causam erro 'cookie too large'
            session.pop('relatorio_data', None)
            
            logger.info(f"Relat√≥rio armazenado com ID: {relatorio_id}")
            
            # Limpa caches antigos
            limpar_cache_antigo()
            
            return render_template('relatorio.html', relatorio=relatorio_final_filtrado, total=len(relatorio_final_filtrado), 
                           relatorio_agrupado=relatorio_agrupado_filtrado, responsaveis=list(relatorio_agrupado_filtrado.keys()))
            
        finally:
            # Remove arquivo tempor√°rio
            try:
                os.unlink(tmp_file_path)
            except:
                pass

    except ValueError as e:
        # S√≥ bloqueia se for erro cr√≠tico (n√£o relacionado a valida√ß√£o de colunas)
        error_msg = str(e)
        if "Colunas obrigat√≥rias" in error_msg or "colunas faltando" in error_msg.lower():
            # Se for erro de colunas, apenas avisa mas continua
            logger.warning(f"Aviso de valida√ß√£o (continuando processamento): {error_msg}")
            flash(f'Aviso: {error_msg}. O sistema continuar√° processando com os dados dispon√≠veis.', 'warning')
            # N√ÉO retorna redirect - continua processamento
        else:
            # Outros erros ValueError s√£o cr√≠ticos
            logger.error(f"Erro cr√≠tico: {error_msg}")
            flash(f'Erro ao processar arquivo: {error_msg}', 'error')
            return redirect(url_for('index'))
    except Exception as e:
        logger.error(f"Erro cr√≠tico ao processar a planilha: {str(e)}")
        flash(f'Erro ao processar arquivo: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/responsavel/<responsavel>')
def ver_responsavel(responsavel):
    """Visualiza√ß√£o individual de an√°lises por Respons√°vel"""
    try:
        # Debug: Log session state
        logger.info(f"Session keys at start: {list(session.keys())}")
        logger.info(f"Requested respons√°vel: '{responsavel}'")
        
        # Carrega dados do cache
        relatorio_final = None
        relatorio_agrupado = None
        
        # Tenta 1: Cache de arquivo
        if 'relatorio_id_atual' in session:
            relatorio_id = session['relatorio_id_atual']
            logger.info(f"Found relatorio_id_atual in session: {relatorio_id}")
            dados_cache = carregar_relatorio_cache(relatorio_id)
            if dados_cache:
                logger.info(f"Cache loaded successfully, type: {type(dados_cache)}")
                # Verifica se √© o formato novo (com dicion√°rio) ou antigo (lista direta)
                if isinstance(dados_cache, dict):
                    relatorio_final = dados_cache.get('relatorio_final', [])
                    relatorio_agrupado = dados_cache.get('relatorio_agrupado', {})
                    logger.info(f"Using new cache format - found {len(relatorio_agrupado)} respons√°veis")
                else:
                    # Formato antigo - converte para novo
                    relatorio_final = dados_cache
                    relatorio_agrupado = {}
                    # Agrupa por respons√°vel
                    for item in relatorio_final:
                        responsavel_item = item.get('responsavel', 'N√£o informado')
                        if responsavel_item not in relatorio_agrupado:
                            relatorio_agrupado[responsavel_item] = []
                        relatorio_agrupado[responsavel_item].append(item)
                    logger.info(f"Converted old cache format - found {len(relatorio_agrupado)} respons√°veis")
            else:
                logger.warning(f"Failed to load cache for ID: {relatorio_id}")
        else:
            logger.warning("relatorio_id_atual not found in session")
        
        # Tenta 2: Sess√£o (backup)
        if not relatorio_agrupado and 'relatorio_data' in session:
            logger.info("Trying backup session data")
            dados_session = session['relatorio_data']
            if isinstance(dados_session, dict):
                relatorio_final = dados_session.get('relatorio_final', [])
                relatorio_agrupado = dados_session.get('relatorio_agrupado', {})
                logger.info(f"Using session backup - found {len(relatorio_agrupado)} respons√°veis")
            else:
                # Formato antigo na sess√£o
                relatorio_final = dados_session
                relatorio_agrupado = {}
                for item in relatorio_final:
                    responsavel_item = item.get('responsavel', 'N√£o informado')
                    if responsavel_item not in relatorio_agrupado:
                        relatorio_agrupado[responsavel_item] = []
                    relatorio_agrupado[responsavel_item].append(item)
                logger.info(f"Converted session backup - found {len(relatorio_agrupado)} respons√°veis")
        
        # Tenta 3: Busca autom√°tica no cache se n√£o encontrou nada
        if not relatorio_agrupado:
            logger.info("Trying auto-discovery in cache files")
            try:
                for filename in os.listdir(CACHE_DIR):
                    if filename.startswith('relatorio_') and filename.endswith('.pkl'):
                        cache_file = os.path.join(CACHE_DIR, filename)
                        try:
                            with open(cache_file, 'rb') as f:
                                cache_data = pickle.load(f)
                            
                            # Verifica se o cache n√£o √© muito antigo (24 horas)
                            if datetime.now() - cache_data['timestamp'] < timedelta(hours=24):
                                dados_cache = cache_data['data']
                                if isinstance(dados_cache, dict) and 'relatorio_agrupado' in dados_cache:
                                    relatorio_agrupado = dados_cache['relatorio_agrupado']
                                    relatorio_final = dados_cache.get('relatorio_final', [])
                                    logger.info(f"Auto-discovered cache with {len(relatorio_agrupado)} respons√°veis")
                                    break
                        except Exception as e:
                            logger.warning(f"Failed to read cache file {filename}: {e}")
                            continue
            except Exception as e:
                logger.error(f"Error in auto-discovery: {e}")
        
        # Debug: Log do que foi encontrado
        logger.info(f"Final result - Cache: {bool(relatorio_agrupado)}, Session: {bool('relatorio_data' in session)}")
        if relatorio_agrupado:
            logger.info(f"Respons√°veis dispon√≠veis: {list(relatorio_agrupado.keys())}")
            logger.info(f"Respons√°vel solicitado: '{responsavel}'")
        
        if not relatorio_agrupado:
            logger.error("Dados agrupados n√£o encontrados em nenhum lugar")
            flash('Dados n√£o encontrados. Por favor, processe a planilha novamente.', 'error')
            return redirect(url_for('index'))
        
        # Busca dados do respons√°vel espec√≠fico
        dados_responsavel = relatorio_agrupado.get(responsavel, [])
        
        if not dados_responsavel:
            logger.warning(f"Respons√°vel '{responsavel}' n√£o encontrado nos dados")
            logger.info(f"Available respons√°veis: {list(relatorio_agrupado.keys())}")
            flash(f'Respons√°vel "{responsavel}" n√£o encontrado nos dados.', 'warning')
            return redirect(url_for('index'))
        
        logger.info(f"Exibindo {len(dados_responsavel)} itens para o respons√°vel: {responsavel}")
        
        return render_template('responsavel.html', 
                             relatorio=dados_responsavel, 
                             total=len(dados_responsavel),
                             responsavel=responsavel,
                             todos_responsaveis=list(relatorio_agrupado.keys()))
        
    except Exception as e:
        logger.error(f"Erro ao visualizar respons√°vel: {str(e)}")
        flash(f'Erro ao carregar dados do respons√°vel: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/gerar_pdf_responsavel/<responsavel>')
def gerar_pdf_responsavel(responsavel):
    """Gera PDF individual para um Respons√°vel espec√≠fico"""
    try:
        logger.info(f"Generating PDF for respons√°vel: '{responsavel}'")
        
        # Carrega dados do cache (usando a mesma l√≥gica de auto-discovery)
        relatorio_agrupado = None
        relatorio_final = None
        
        # Tenta 1: Cache de arquivo
        if 'relatorio_id_atual' in session:
            relatorio_id = session['relatorio_id_atual']
            logger.info(f"Found relatorio_id_atual in session: {relatorio_id}")
            dados_cache = carregar_relatorio_cache(relatorio_id)
            if dados_cache:
                logger.info(f"Cache loaded successfully for PDF")
                # Verifica se √© o formato novo (com dicion√°rio) ou antigo (lista direta)
                if isinstance(dados_cache, dict):
                    relatorio_agrupado = dados_cache.get('relatorio_agrupado', {})
                    relatorio_final = dados_cache.get('relatorio_final', [])
                else:
                    # Formato antigo - agrupa por respons√°vel
                    relatorio_final = dados_cache
                    relatorio_agrupado = {}
                    for item in relatorio_final:
                        responsavel_item = item.get('responsavel', 'N√£o informado')
                        if responsavel_item not in relatorio_agrupado:
                            relatorio_agrupado[responsavel_item] = []
                        relatorio_agrupado[responsavel_item].append(item)
            else:
                logger.warning(f"Failed to load cache for PDF with ID: {relatorio_id}")
        else:
            logger.warning("relatorio_id_atual not found in session for PDF")
        
        # Tenta 2: Sess√£o (backup)
        if not relatorio_agrupado and 'relatorio_data' in session:
            logger.info("Trying backup session data for PDF")
            dados_session = session['relatorio_data']
            if isinstance(dados_session, dict):
                relatorio_agrupado = dados_session.get('relatorio_agrupado', {})
                relatorio_final = dados_session.get('relatorio_final', [])
            else:
                # Formato antigo na sess√£o
                relatorio_final = dados_session
                relatorio_agrupado = {}
                for item in relatorio_final:
                    responsavel_item = item.get('responsavel', 'N√£o informado')
                    if responsavel_item not in relatorio_agrupado:
                        relatorio_agrupado[responsavel_item] = []
                    relatorio_agrupado[responsavel_item].append(item)
        
        # Tenta 3: Busca autom√°tica no cache se n√£o encontrou nada
        if not relatorio_agrupado:
            logger.info("Trying auto-discovery in cache files for PDF")
            try:
                for filename in os.listdir(CACHE_DIR):
                    if filename.startswith('relatorio_') and filename.endswith('.pkl'):
                        cache_file = os.path.join(CACHE_DIR, filename)
                        try:
                            with open(cache_file, 'rb') as f:
                                cache_data = pickle.load(f)
                            
                            # Verifica se o cache n√£o √© muito antigo (24 horas)
                            if datetime.now() - cache_data['timestamp'] < timedelta(hours=24):
                                dados_cache = cache_data['data']
                                if isinstance(dados_cache, dict) and 'relatorio_agrupado' in dados_cache:
                                    relatorio_agrupado = dados_cache['relatorio_agrupado']
                                    relatorio_final = dados_cache.get('relatorio_final', [])
                                    logger.info(f"Auto-discovered cache for PDF with {len(relatorio_agrupado)} respons√°veis")
                                    break
                        except Exception as e:
                            logger.warning(f"Failed to read cache file {filename} for PDF: {e}")
                            continue
            except Exception as e:
                logger.error(f"Error in auto-discovery for PDF: {e}")
        
        if not relatorio_agrupado:
            logger.error("Dados agrupados n√£o encontrados para PDF")
            flash('Dados n√£o encontrados. Por favor, processe a planilha novamente.', 'error')
            return redirect(url_for('index'))
        
        # Busca dados do respons√°vel espec√≠fico
        dados_responsavel = relatorio_agrupado.get(responsavel, [])
        
        if not dados_responsavel:
            logger.warning(f"Respons√°vel '{responsavel}' n√£o encontrado para PDF")
            logger.info(f"Available respons√°veis for PDF: {list(relatorio_agrupado.keys())}")
            flash(f'Respons√°vel "{responsavel}" n√£o encontrado nos dados.', 'warning')
            return redirect(url_for('processar'))
        
        total = len(dados_responsavel)
        logger.info(f"Gerando PDF individual para {responsavel}: {total} itens")
        
        # Cria buffer em mem√≥ria
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=50, leftMargin=50, topMargin=50, bottomMargin=50)
        
        # Estilos
        styles = getSampleStyleSheet()
        
        # Estilo do T√≠tulo Principal
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=30,
            alignment=1, # Center
            textColor=VM_GREEN,
            fontName='Helvetica-Bold'
        )
        
        # Estilo para Nome do Neg√≥cio
        business_style = ParagraphStyle(
            'BusinessTitle',
            parent=styles['Heading2'],
            fontSize=16,
            spaceAfter=10,
            spaceBefore=20,
            textColor=VM_ORANGE,
            fontName='Helvetica-Bold',
            borderPadding=5,
            borderColor=VM_GREEN,
            borderWidth=0,
            backColor=colors.Color(0.95, 0.95, 0.95) # Fundo cinza claro
        )
        
        # Estilo para Subt√≠tulos (Diagn√≥stico, Estrat√©gia, etc)
        section_header_style = ParagraphStyle(
            'SectionHeader',
            parent=styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            spaceBefore=12,
            textColor=VM_GREEN,
            fontName='Helvetica-Bold'
        )
        
        # Estilo Normal
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            leading=14,
            textColor=colors.HexColor('#2c3e50')
        )
        
        # Estilo para Labels (Empresa, Respons√°vel, etc)
        label_style = ParagraphStyle(
            'LabelStyle',
            parent=styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#7f8c8d'),
            leading=14
        )

        # Conte√∫do do PDF
        story = []
        
        # T√≠tulo
        story.append(Paragraph(f"Relat√≥rio Individual - {responsavel}", title_style))
        story.append(Paragraph(f"Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}", 
                             ParagraphStyle('Date', parent=normal_style, alignment=1, textColor=colors.gray)))
        story.append(Paragraph(f"Total de Neg√≥cios: {total}", 
                             ParagraphStyle('Total', parent=normal_style, alignment=1, textColor=colors.gray)))
        story.append(Spacer(1, 30))
        
        # An√°lises detalhadas
        for i, item in enumerate(dados_responsavel, 1):
            # Container para manter o bloco junto se poss√≠vel
            elements = []
            
            # Cabe√ßalho do Cliente
            elements.append(Paragraph(f"{i}. {item['negocio']}", business_style))
            
            # Dados principais em tabela para organiza√ß√£o
            data = [
                [Paragraph(f"<b>Empresa:</b> {item['empresa']}", normal_style),
                 Paragraph(f"<b>Fase:</b> {item['fase']}", normal_style)],
                [Paragraph(f"<b>Temperatura:</b> {item.get('temperatura_atual', 'N√£o informada')}", normal_style),
                 Paragraph(f"<b>√öltimo Follow-up:</b> #{item.get('ultimo_follow', 0)}", normal_style)],
                [Paragraph(f"<b>Pr√≥ximo Passo:</b> #{item.get('proximo_follow', 1)}", normal_style),
                 Paragraph("", normal_style)]
            ]
            
            t = Table(data, colWidths=[3.5*inch, 3.5*inch])
            t.setStyle(TableStyle([
                ('VALIGN', (0,0), (-1,-1), 'TOP'),
                ('TOPPADDING', (0,0), (-1,-1), 2),
                ('BOTTOMPADDING', (0,0), (-1,-1), 2),
            ]))
            elements.append(t)
            elements.append(Spacer(1, 10))
            
            # Processamento da An√°lise da IA
            analise_text = item.get('analise_proximo_passo', '')
            
            # Remove os marcadores de markdown ** se existirem
            analise_text = analise_text.replace('**DIAGN√ìSTICO DA SITUA√á√ÉO:**', 'DIAGN√ìSTICO DA SITUA√á√ÉO')
            analise_text = analise_text.replace('**ESTRAT√âGIA PARA O PR√ìXIMO PASSO:**', 'ESTRAT√âGIA PARA O PR√ìXIMO PASSO')
            analise_text = analise_text.replace('**A√á√ÉO RECOMENDADA:**', 'A√á√ÉO RECOMENDADA')
            
            # Divide o texto em linhas para processar
            lines = analise_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Verifica se √© um cabe√ßalho de se√ß√£o
                if 'DIAGN√ìSTICO DA SITUA√á√ÉO' in line:
                    elements.append(Paragraph("üîç DIAGN√ìSTICO DA SITUA√á√ÉO", section_header_style))
                elif 'ESTRAT√âGIA PARA O PR√ìXIMO PASSO' in line:
                    elements.append(Paragraph("üéØ ESTRAT√âGIA PARA O PR√ìXIMO PASSO", section_header_style))
                elif 'A√á√ÉO RECOMENDADA' in line:
                    elements.append(Paragraph("üöÄ A√á√ÉO RECOMENDADA", section_header_style))
                else:
                    # Remove asteriscos de markdown se sobrarem
                    clean_line = line.replace('**', '').strip()
                    if clean_line.startswith('-'):
                        # Item de lista
                        elements.append(Paragraph(f"‚Ä¢ {clean_line[1:].strip()}", normal_style))
                    else:
                        elements.append(Paragraph(clean_line, normal_style))
            
            elements.append(Spacer(1, 20))
            
            # Adiciona ao story (tenta manter junto)
            story.append(KeepTogether(elements))
            
            # Linha divis√≥ria
            if i < total:
                story.append(Spacer(1, 10))
                story.append(Paragraph("_" * 60, ParagraphStyle('Line', parent=normal_style, alignment=1, textColor=colors.lightgrey)))
                story.append(Spacer(1, 20))
        
        # Rodap√©
        story.append(Spacer(1, 30))
        story.append(Paragraph(f"Relat√≥rio individual gerado para: {responsavel}", 
                             ParagraphStyle('Footer', parent=normal_style, alignment=1, fontSize=8, textColor=colors.gray)))
        story.append(Paragraph("Relat√≥rio gerado por Sistema de Automa√ß√£o de Vendas", 
                             ParagraphStyle('Footer', parent=normal_style, alignment=1, fontSize=8, textColor=colors.gray)))
        
        # Gera o PDF
        doc.build(story)
        buffer.seek(0)
        
        # Prepara resposta
        response = make_response(buffer.getvalue())
        response.headers['Content-Type'] = 'application/pdf'
        response.headers['Content-Disposition'] = f'inline; filename=relatorio_{responsavel.replace(" ", "_")}_{datetime.now().strftime("%Y%m%d_%H%M")}.pdf'
        
        logger.info(f"PDF individual gerado com sucesso para {responsavel}: {total} itens")
        return response
        
    except Exception as e:
        logger.error(f"Erro ao gerar PDF individual: {str(e)}")
        flash(f'Erro ao gerar PDF individual: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/gerar_pdf')
def gerar_pdf():
    """Gera PDF profissional do relat√≥rio de an√°lises"""
    try:
        relatorio_final = None
        
        # Debug: Log session contents
        logger.info(f"Session keys: {list(session.keys())}")
        
        # Tenta carregar do cache de arquivo primeiro (prioridade m√°xima)
        if 'relatorio_id_atual' in session:
            relatorio_id = session['relatorio_id_atual']
            relatorio_final = carregar_relatorio_cache(relatorio_id)
            if relatorio_final:
                logger.info(f"Relat√≥rio carregado do cache de arquivo: {len(relatorio_final)} itens")
        
        # Se n√£o encontrou pelo ID atual, tenta outros m√©todos
        if not relatorio_final and 'relatorios' in session and session['relatorios']:
            # Tenta o √∫ltimo ID da sess√£o (fallback)
            ultimo_id = list(session['relatorios'].keys())[-1]
            if isinstance(session['relatorios'][ultimo_id], str):
                # Se for string, √© um ID de cache
                relatorio_final = carregar_relatorio_cache(session['relatorios'][ultimo_id])
            else:
                # Se for lista diretamente (modo antigo)
                relatorio_final = session['relatorios'][ultimo_id]
            
            if relatorio_final:
                logger.info(f"Usando relatorios da sess√£o: {len(relatorio_final)} itens")
        
        # Se ainda n√£o encontrou, tenta obter da sess√£o (apenas como √∫ltimo recurso)
        if not relatorio_final and 'relatorio_data' in session and session['relatorio_data']:
            relatorio_final = session['relatorio_data']
            logger.warning(f"Usando relatorio_data limitado da sess√£o: {len(relatorio_final)} itens (backup apenas)")
        
        # Se ainda n√£o encontrou, tenta encontrar o cache mais recente
        if not relatorio_final:
            try:
                cache_files = [f for f in os.listdir(CACHE_DIR) if f.startswith('relatorio_') and f.endswith('.pkl')]
                if cache_files:
                    # Pega o arquivo mais recente
                    cache_files.sort(key=lambda x: os.path.getmtime(os.path.join(CACHE_DIR, x)), reverse=True)
                    latest_cache = cache_files[0]
                    relatorio_id = latest_cache.replace('relatorio_', '').replace('.pkl', '')
                    relatorio_final = carregar_relatorio_cache(relatorio_id)
                    if relatorio_final:
                        logger.info(f"Usando cache mais recente encontrado: {relatorio_id}")
            except Exception as e:
                logger.warning(f"Erro ao buscar cache mais recente: {str(e)}")
        
        if not relatorio_final:
            logger.error("Dados do relat√≥rio n√£o encontrados na sess√£o nem no cache")
            logger.error(f"Session data: {dict(session)}")
            flash('Dados do relat√≥rio n√£o encontrados. Por favor, processe a planilha novamente.', 'error')
            return redirect(url_for('index'))
        
        total = len(relatorio_final)
        logger.info(f"Gerando PDF para {total} itens")
        
        # Cria buffer em mem√≥ria
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=50, leftMargin=50, topMargin=50, bottomMargin=50)
        
        # Estilos
        styles = getSampleStyleSheet()
        
        # Estilo do T√≠tulo Principal
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=24,
            spaceAfter=30,
            alignment=1, # Center
            textColor=VM_GREEN,
            fontName='Helvetica-Bold'
        )
        
        # Estilo para Nome do Neg√≥cio
        business_style = ParagraphStyle(
            'BusinessTitle',
            parent=styles['Heading2'],
            fontSize=16,
            spaceAfter=10,
            spaceBefore=20,
            textColor=VM_ORANGE,
            fontName='Helvetica-Bold',
            borderPadding=5,
            borderColor=VM_GREEN,
            borderWidth=0,
            backColor=colors.Color(0.95, 0.95, 0.95) # Fundo cinza claro
        )
        
        # Estilo para Subt√≠tulos (Diagn√≥stico, Estrat√©gia, etc)
        section_header_style = ParagraphStyle(
            'SectionHeader',
            parent=styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            spaceBefore=12,
            textColor=VM_GREEN,
            fontName='Helvetica-Bold'
        )
        
        # Estilo Normal
        normal_style = ParagraphStyle(
            'CustomNormal',
            parent=styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            leading=14,
            textColor=colors.HexColor('#2c3e50')
        )
        
        # Estilo para Labels (Empresa, Respons√°vel, etc)
        label_style = ParagraphStyle(
            'LabelStyle',
            parent=styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#7f8c8d'),
            leading=14
        )

        # Conte√∫do do PDF
        story = []
        
        # T√≠tulo
        story.append(Paragraph("Relat√≥rio de An√°lise Estrat√©gica", title_style))
        story.append(Paragraph(f"Gerado em: {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}", 
                             ParagraphStyle('Date', parent=normal_style, alignment=1, textColor=colors.gray)))
        story.append(Spacer(1, 30))
        
        # An√°lises detalhadas
        for i, item in enumerate(relatorio_final, 1):
            # Container para manter o bloco junto se poss√≠vel
            elements = []
            
            # Cabe√ßalho do Cliente
            elements.append(Paragraph(f"{i}. {item['negocio']}", business_style))
            
            # Dados principais em tabela para organiza√ß√£o
            data = [
                [Paragraph(f"<b>Empresa:</b> {item['empresa']}", normal_style),
                 Paragraph(f"<b>Respons√°vel:</b> {item['responsavel']}", normal_style)],
                [Paragraph(f"<b>Fase:</b> {item['fase']}", normal_style),
                 Paragraph(f"<b>Temperatura:</b> {item.get('temperatura_atual', 'N√£o informada')}", normal_style)],
                [Paragraph(f"<b>√öltimo Follow-up:</b> #{item.get('ultimo_follow', 0)}", normal_style),
                 Paragraph(f"<b>Pr√≥ximo Passo:</b> #{item.get('proximo_follow', 1)}", normal_style)]
            ]
            
            t = Table(data, colWidths=[3.5*inch, 3.5*inch])
            t.setStyle(TableStyle([
                ('VALIGN', (0,0), (-1,-1), 'TOP'),
                ('TOPPADDING', (0,0), (-1,-1), 2),
                ('BOTTOMPADDING', (0,0), (-1,-1), 2),
            ]))
            elements.append(t)
            elements.append(Spacer(1, 10))
            
            # Processamento da An√°lise da IA
            analise_text = item.get('analise_proximo_passo', '')
            
            # Remove os marcadores de markdown ** se existirem
            analise_text = analise_text.replace('**DIAGN√ìSTICO DA SITUA√á√ÉO:**', 'DIAGN√ìSTICO DA SITUA√á√ÉO')
            analise_text = analise_text.replace('**ESTRAT√âGIA PARA O PR√ìXIMO PASSO:**', 'ESTRAT√âGIA PARA O PR√ìXIMO PASSO')
            analise_text = analise_text.replace('**A√á√ÉO RECOMENDADA:**', 'A√á√ÉO RECOMENDADA')
            
            # Divide o texto em linhas para processar
            lines = analise_text.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # Verifica se √© um cabe√ßalho de se√ß√£o
                if 'DIAGN√ìSTICO DA SITUA√á√ÉO' in line:
                    elements.append(Paragraph("üîç DIAGN√ìSTICO DA SITUA√á√ÉO", section_header_style))
                elif 'ESTRAT√âGIA PARA O PR√ìXIMO PASSO' in line:
                    elements.append(Paragraph("üéØ ESTRAT√âGIA PARA O PR√ìXIMO PASSO", section_header_style))
                elif 'A√á√ÉO RECOMENDADA' in line:
                    elements.append(Paragraph("üöÄ A√á√ÉO RECOMENDADA", section_header_style))
                else:
                    # Remove asteriscos de markdown se sobrarem
                    clean_line = line.replace('**', '').strip()
                    if clean_line.startswith('-'):
                        # Item de lista
                        elements.append(Paragraph(f"‚Ä¢ {clean_line[1:].strip()}", normal_style))
                    else:
                        elements.append(Paragraph(clean_line, normal_style))
            
            elements.append(Spacer(1, 20))
            
            # Adiciona ao story (tenta manter junto)
            story.append(KeepTogether(elements))
            
            # Linha divis√≥ria
            if i < total:
                story.append(Spacer(1, 10))
                story.append(Paragraph("_" * 60, ParagraphStyle('Line', parent=normal_style, alignment=1, textColor=colors.lightgrey)))
                story.append(Spacer(1, 20))
        
        # Rodap√©
        story.append(Spacer(1, 30))
        story.append(Paragraph("Relat√≥rio gerado por Sistema de Automa√ß√£o de Vendas", 
                             ParagraphStyle('Footer', parent=normal_style, alignment=1, fontSize=8, textColor=colors.gray)))
        
        # Gera o PDF
        doc.build(story)
        buffer.seek(0)
        
        # Prepara resposta
        response = make_response(buffer.getvalue())
        response.headers['Content-Type'] = 'application/pdf'
        response.headers['Content-Disposition'] = f'inline; filename=relatorio_estrategico_{datetime.now().strftime("%Y%m%d_%H%M")}.pdf'
        
        logger.info(f"PDF gerado com sucesso: {total} itens")
        return response
        
    except Exception as e:
        logger.error(f"Erro ao gerar PDF: {str(e)}")
        flash(f'Erro ao gerar PDF: {str(e)}', 'error')
        return redirect(url_for('index'))


@app.route('/debug_session')
def debug_session():
    """Debug route to check session status"""
    session_info = {
        'keys': list(session.keys()),
        'relatorio_data_exists': 'relatorio_data' in session,
        'relatorios_exists': 'relatorios' in session,
        'relatorio_data_length': len(session.get('relatorio_data', [])) if 'relatorio_data' in session else 0,
        'relatorios_count': len(session.get('relatorios', {})) if 'relatorios' in session else 0,
    }
    return session_info


@app.route('/limpar_sessao')
def limpar_sessao():
    """Limpa dados da sess√£o para permitir novos processamentos"""
    try:
        keys_to_clear = ['relatorio_data', 'relatorios', 'relatorio_id_atual']
        for key in keys_to_clear:
            if key in session:
                session.pop(key, None)
        
        logger.info("Sess√£o de relat√≥rios limpa com sucesso")
        flash('Sess√£o limpa. Voc√™ pode processar novas planilhas agora.', 'success')
        
    except Exception as e:
        logger.error(f"Erro ao limpar sess√£o: {str(e)}")
        flash(f'Erro ao limpar sess√£o: {str(e)}', 'error')
    
    return redirect(url_for('index'))


# ==================== INICIALIZA√á√ÉO ====================

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('FLASK_DEBUG', 'True').lower() == 'true'
    
    logger.info(f"Iniciando servidor Flask na porta {port} (debug={debug})")
    logger.info(f"Usando API Groq com modelo: {GROQ_MODEL}")
    app.run(debug=debug, port=port)
